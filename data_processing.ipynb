{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KC-ML2 TAKE-HOME PROJECT\n",
    "\n",
    "## Show and Tell: A Neural Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple paper review\n",
    "## 1. Introduction\n",
    "#### Task: Automatically describing the content of an image\n",
    "__Visual understanding__ 분야는 기존의 Computer vision 문제들, Image classification 혹은 Object recognition 보다 어렵다.  \n",
    "이미지를 문장으로 설명해내는 Task는 이미지에 포함된 객체와 함께 그것들의 특성과 행동까지 잡아내야 하며, 이를 자연어로 표현해야 하는데, 이는 언어모델까지 필요하다.  \n",
    "저자들은 이 Task를 풀기 위해 __Machine translation__으로부터 영감을 얻었다.  \n",
    "문장 내의 단어들을 해석, 치환, 재배치 하는 기존의 방식에서, 최근 __RNN__을 이용한 __Encoder-Decoder__ 방식이 훨씬 간단하고 좋은 성능으로 해결하고 있다.  \n",
    "__Encoder-RNN__이 문장을 적절한 __Vector representation__으로 변환하고, __Decoder-RNN__이 __vector__ 로부터 번역한 문장을 생성한다.  \n",
    "저자들은 이 __Nerual Image Caption__(or __NIC__)라 불리는 __Encoder-RNN__을 __Encoder-CNN__으로 대체한 모델을 제안한다.\n",
    "\n",
    "Figure 1. 제안한 NIC model의 개요   \n",
    "![Figure1](resources/figure1.png \"figure1\")\n",
    "\n",
    "#### Proposed Model: CNN encoder and RNN decoder(like machine translation), NIC\n",
    "__CNN__은 기존의 Computer vision 문제들에서 우수한 성능을 내고 있고, 이는 __CNN__이 이미지를 잘 __embeddeing__ 한다는 것에 설득력이 있다.  \n",
    "따라서 __CNN__을 이미지 encoder로 사용하는 것은 자연스러운 일이고, 저자들은 __ImageNet__으로 __pre-trained__ 된 모델을 사용했다.  \n",
    "__NIC__는 다른 __NN__과 같이 SGD를 통해 학습된다.  \n",
    "__NIC__는 __Pascal, Flicker8k, 30k, MSCOCO, SBU dataset__에서 사람과 근접한 __State of the art__를 달성했다.\n",
    "\n",
    "## 3. Model\n",
    "__Machine translation__과 마찬가지로 __Encoder__는 고정된 차원의 __vector__로 __encoding__ 하고,  \n",
    "__Decoder__는 해당 __vector__를 __decoding__하여 이미지를 설명하는 문장을 생성한다.  \n",
    "당시 연구들은 __sequence model__이 주어졌을 때 __correct translation__의 확률을 __maximize__하는 방향으로 학습하는 것이 좋다고 통계적으로 보여진다.  \n",
    "저자들은 아래와 같은 수식을 __Maximizing__하는 방식으로 모델을 학습시켰다.\n",
    "\n",
    "Equation 1. Image __*I*__ 가 주어졌을 때, Description __*S*__  \n",
    "![Eqation1](resources/equation1.png \"equation1\")\n",
    "\n",
    "문장 __*S*__ 는 길이가 제한적이지 않고, 따라서 __joint probability__를 이용한 아래 수식처럼 표현할 수 있다.\n",
    "\n",
    "Equation 2. Joint probability  \n",
    "![Eqation2](resources/equation2.png \"equation2\")\n",
    "\n",
    "문장을 생성하는 모델에 __RNN__을 사용하는것은 자연스럽고, 저자들은 RNN을 더 Concrete하게 만들기 위해 두 가지 Crucial한 선택을 했다.  \n",
    "\n",
    "#### 1. 어떤 Non-linear function을 써야 학습이 잘 될 것인가.  \n",
    "이에 대한 문제를 __LSTM__을 사용하여 해결하였다.  \n",
    "__LSTM__은 __Vanishing or exploding gradient__ 문제를 잘 해결하기 때문에 당시 __sequence task__에서 __State of the art__를 달성하였고, 저자들도 이를 선택했다.   \n",
    "\n",
    "#### 2. 어떻게 이미지와 단어를 동시에 입력으로 넣어줄 수 있을까.  \n",
    "2014 이미지넷에서 우승한 __GoogLeNet__을 사용하여 이미지를 __embedding__ 했다.  \n",
    "기존의 Computer vision 문제를 잘 해결하는 모델이 이미지를 잘 표현하는 __vector representation__를 만들 것이라 생각했다.  \n",
    "또한 __문장을 word 단위로 split__ 하여 __Image vector__와 같은 차원으로 __embedding__ 했다. \n",
    "\n",
    "### 3.1. LSTM based Sentence Generator\n",
    "__LSTM__은 __Vanishing or exploding gradient__ 문제를 잘 해결하기 때문에 선택했고, __LSTM__의 전체적인 구조는 아래와 같다.  \n",
    "\n",
    "Figure 2. LSTM structure  \n",
    "![Figure2](resources/figure2.png \"figure2\")\n",
    "\n",
    "__Encoder-CNN__과 결합한 LSTM의 모습은 아래와 같고, 모든 LSTM의 parameter는 공유된다.\n",
    "\n",
    "Figure 3. LSTM model combined with a CNN image embedder  \n",
    "![Figure3](resources/figure3.png \"figure3\")\n",
    "\n",
    "이미지는 맨 처음 입력 단 한번만 들어가고, 이미지 벡터로부터 LSTM이 출력한 결과를 다음 LSTM의 입력으로 넣으면서 학습, 추론한다.  \n",
    "저자들은 매 step마다 이미지를 넣어주는 시도를 했으나 이는 오히려 더 쉽게 Overfit 되는 결과를 보였다.  \n",
    "\n",
    "#### Inference\n",
    "저자들은 __NIC__에서 __Inference__하는 두 가지 방법을 제시했다.  \n",
    "\n",
    "##### Sampling\n",
    "__Sampling__ 방식은 아주 단순하다.  \n",
    "최대 문장의 길이가 될 때 혹은 끝나는 신호가 나올 때까지 __LSTM__에서 __Softmax__를 거쳐 출력된 최대 값의 단어를 이어붙여, 이를 다음 LSTM의 입력으로 넣어주는 방식이다.\n",
    "\n",
    "##### BeamSearch\n",
    "__BeamSearch__는 매 t번째까지의 입력으로 만들어진 문장 k개를 유지하며, k개의 문장들로부터 t+1번째까지의 문장 중 다시 k개를 반환하는 방식이다.  \n",
    "k=1 일 때는 굉장히 Greedy하며 BLEU score는 k=20일 때의 BLEU score보다 평균적으로 2점 정도 하락했다.  \n",
    " \n",
    "## 4. Experiments\n",
    "### 4.1 Evaluation Metrics\n",
    "__Image description__에서 가장 많이 사용되는 __Metric__은 __BLEU score__이고, __n-gram__을 통해 평가된다.  \n",
    "__n-gram__이란 다음에 나올 단어를 예측할 때 은 앞선 __n-1__개의 단어에 의존하는 방식이다.  \n",
    "주로 __1-gram__을 많이 이용하고 저자들 또한 __BLEU-1__을 주로 이용하였고, 추가적으로 __ME-TEOR, Cider score__도 제시하였다.\n",
    "\n",
    "### 4.2. Datasets\n",
    "\n",
    "다음과 같은 Dataset을 이용하였다.  \n",
    "\n",
    "Table 1. Datasets.  \n",
    "![table1](resources/table1.png \"table1\")\n",
    "\n",
    "__SBU__를 제외하고는 __모두 5개의 문장__이 __Labeling__ 되어 있다.  \n",
    "저자들은 __SBU__가 __Flikcr__에서 사용자들이 올린 Description이기 때문에 Noise가 있다고 보았다.  \n",
    "또한 __Pascal__은 Test를 위해서만 사용하였는데, 나머지 4개의 Data로 학습을 하고 Testing 하였다.  \n",
    "\n",
    "### 4.3. Result\n",
    "#### 4.3.1. Training Details\n",
    "\n",
    "논문에서 저자들은 아래와 같은 사항들을 이용하여 실험했다.\n",
    "\n",
    "##### 1. 학습 시 __CNN__은 __Imagnet__을 통해 __pre-trained__ 된 __weight__를 그대로 이용하였고, __fine tuning__은 하지 않았다.  \n",
    "##### 2. __Word embedding vector__도 __pre-trained__ 된 모델을 써 보았으나 효과가 없었다.  \n",
    "##### 3. __SGD__로 학습했고 __fixed learning rate__를 사용, __decay__는 사용하지 않았다.  \n",
    "##### 4. __Word embedding size__와 LSTM의 크기는 512로 셋팅했다.  \n",
    "##### 5. __Overfitting__을 피하기 위해 __Dropout__과 __ensemble__을 사용했다. - BLEU score 향상은 거의 없었다.  \n",
    "##### 6. __Hidden layer__의 개수와 깊이를 다양하게 설정했다.  \n",
    "\n",
    "#### 4.3.2. Generation Results\n",
    "\n",
    "Table 2. BLEU-1 score.  \n",
    "![table2](resources/table2.png \"table2\")\n",
    "\n",
    "사용한 4가지 Datasets 전부에서 __SOTA BLEU-1 score__를 갱신했다.  \n",
    "\n",
    "#### 4.3.3. Transfer Learning, Data Size and Label Quality\n",
    "\n",
    "어떤 __datasets__로부터 학습된 모델은 다른 __datasets__에도 적용될 수 있는지 실험했다.  \n",
    "같은 유저 집단이 만든 __Flickr dataset__에서는 __Transfer learning__이 효과가 있었다.  \n",
    "__Flickr30k__로 학습된 모델을 이용하면 __Flickr8k__에서 __BLEU score__가 4점 정도 상승한다.  \n",
    "__MSCOCO__는 __Flickr30k__ 보다 5배 크지만, __dataset 간 __miss-match__가 많았고 __BLEU score__가 10점 정도 내려갔다.  \n",
    "\n",
    "#### 4.3.4. Generation Diversity Discussion\n",
    "\n",
    "저자들은 __Generate model__ 관점에서, 얼마나 다양한 문장들을 생성할 수 있는지, 얼마나 수준이 높은지를 확인하였다.  \n",
    "__k=20__인 __BeamSearch__에서 상위 15개 정도는 __BLEU-1 score__ 기준으로 사람과 견줄만한 __58__점 정도를 달성했다.  \n",
    "\n",
    "Table 3. MSCOCO test set의 생성된 몇가지 예시.  \n",
    "![table3](resources/table3.png \"table3\")\n",
    "\n",
    "#### 4.3.7. Analysis of Embeddings\n",
    "__one-hot encoding__과 다르게 __Embedding__은 __Word dictionary__의 크기에 제한되지 않는다.  \n",
    "저자들은 __Embedding space__에서 __KNN__을 이용한 몇 가지 예시를 제시했는데 아래와 같다.  \n",
    "\n",
    "Table 6. KNN을 이용한 Word embedding space analysis.  \n",
    "![table6](resources/table6.png \"table6\")\n",
    "\n",
    "__Embedding Vector__는 다른 __Vision component__에도 도움을 줄 수 있는데,  \n",
    "__horse__와 __pony, donkey__는 근접하기 때문에 __CNN__이 __horse-looking__ 동물의 특징을 추출하는 것이 더 수월해질 것이다.  \n",
    "아주 심한 경우에 __Unicorn__ 같은 몇몇 예시들도 __horse__와 근접하기 때문에, 기존의 __bag of word__ 방식보다 더 많은 정보를 줄 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join, isdir, isfile, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kc-ml2 Take-home project\n",
      "['3342309960_c694b2cce9.jpg', '3234375022_1464ea7f8a.jpg', '1821238649_2fda79d6d7.jpg', '3177468217_56a9142e46.jpg', '337647771_3b819feaba.jpg']\n"
     ]
    }
   ],
   "source": [
    "meta_info = {\n",
    "    'projcet_name': 'kc-ml2 Take-home project',\n",
    "    'image_dir': 'Flicker8k_Dataset/',\n",
    "    'train_list': 'Flickr8k_text/Flickr_8k.trainImages.txt',\n",
    "    'dev_list': 'Flickr8k_text/Flickr_8k.devImages.txt',\n",
    "    'test_list': 'Flickr8k_text/Flickr_8k.testImages.txt',\n",
    "    'text_dir': 'Flickr8k_text/'\n",
    "}\n",
    "\n",
    "print(meta_info['projcet_name'])\n",
    "print(listdir(meta_info['image_dir'])[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" feature extract CNN model\n",
    "This paper used GoogLeNet (InceptionV1) which got good grades in ImageNet 2014\n",
    "but for convenience of implementation, I used various models including InceptionV3 in built-in module of keras.\n",
    "My model has the best performance at VGG19.\n",
    "\"\"\"\n",
    "def model_select(model_name):\n",
    "    if model_name == 'VGG16':\n",
    "        from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "        model = VGG16() # 4096\n",
    "    elif model_name == 'VGG19':\n",
    "        from keras.applications.vgg19 import VGG19, preprocess_input \n",
    "        model = VGG19() # 4096\n",
    "    elif model_name == 'ResNet50':\n",
    "        from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "        model = ResNet50() # 4096\n",
    "    elif model_name == 'InceptionV3':\n",
    "        from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "        model = InceptionV3() # 2048,\n",
    "    elif model_name == 'InceptionResNetV2':\n",
    "        from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "        model = InceptionResNetV2() # 1536,\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 139,570,240\n",
      "Trainable params: 139,570,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'VGG19'\n",
    "base_model = model_select(model_name)\n",
    "# using FC2 layer output\n",
    "cnn_model = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Usually training set is the bigger,\n",
    "so I prefer to testing with validation set first.\n",
    "\"\"\"\n",
    "\n",
    "dev_features = {}\n",
    "dev_h5 = 'dev_features.h5'\n",
    "with h5py.File(dev_h5, 'w') as h5f:\n",
    "    with open(meta_info['dev_list']) as f:\n",
    "        c = 0 # count\n",
    "        contents = f.read()\n",
    "        for line in contents.split('\\n'):\n",
    "            if line == '': # last line or error line\n",
    "                print(c)\n",
    "                continue\n",
    "            if c % 100 == 0:\n",
    "                print(c)\n",
    "            # Unlike other models, inception models use the larger image sizes.\n",
    "            if model_name.find('Inception') != -1:\n",
    "                target_size = (299, 299)\n",
    "            else:\n",
    "                target_size = (224, 224)\n",
    "                \n",
    "            img_path = line\n",
    "            img = load_img(meta_info['image_dir'] + img_path, target_size=target_size)\n",
    "            img = img_to_array(img)\n",
    "            img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "            img = preprocess_input(img)\n",
    "            feature = cnn_model.predict(img)\n",
    "            h5f.create_dataset(img_path.split('.')[0], data=feature)\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        0.        1.0180278 ... 0.        0.        0.       ]]\n",
      "(1, 4096)\n"
     ]
    }
   ],
   "source": [
    "# feature test\n",
    "with h5py.File('dev_features.h5', 'r') as h5f:\n",
    "    print(h5f['2090545563_a4e66ec76b'][:])\n",
    "    print(h5f['2090545563_a4e66ec76b'][:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "train_features = {}\n",
    "train_h5 = 'train_features.h5'\n",
    "with h5py.File(train_h5, 'w') as h5f:\n",
    "    with open(meta_info['train_list']) as f:\n",
    "        c = 0 # count\n",
    "        contents = f.read()\n",
    "        for line in contents.split('\\n'):\n",
    "            if line == '': # last line or error line\n",
    "                print(c)\n",
    "                continue\n",
    "            if c % 1000 == 0:\n",
    "                print(c)\n",
    "\n",
    "            if model_name.find('Inception') != -1:\n",
    "                target_size = (299, 299)\n",
    "            else:\n",
    "                target_size = (224, 224)\n",
    "                \n",
    "            img_path = line\n",
    "            img = load_img(meta_info['image_dir'] + img_path, target_size=target_size)\n",
    "            img = img_to_array(img)\n",
    "            img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "            img = preprocess_input(img)\n",
    "            feature = cnn_model.predict(img)\n",
    "            h5f.create_dataset(img_path.split('.')[0], data=feature)\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "test_features = {}\n",
    "test_h5 = 'test_features.h5'\n",
    "with h5py.File(test_h5, 'w') as h5f:\n",
    "    with open(meta_info['test_list']) as f:\n",
    "        c = 0 # count\n",
    "        contents = f.read()\n",
    "        for line in contents.split('\\n'):\n",
    "            if line == '': # last line or error line\n",
    "                print(c)\n",
    "                continue\n",
    "            if c % 100 == 0:\n",
    "                print(c)\n",
    "\n",
    "            if model_name.find('Inception') != -1:\n",
    "                target_size = (299, 299)\n",
    "            else:\n",
    "                target_size = (224, 224)\n",
    "                \n",
    "            img_path = line\n",
    "            img = load_img(meta_info['image_dir'] + img_path, target_size=target_size)\n",
    "            img = img_to_array(img)\n",
    "            img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "            img = preprocess_input(img)\n",
    "            feature = cnn_model.predict(img)\n",
    "            h5f.create_dataset(img_path.split('.')[0], data=feature)\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "40460\n",
      "number of images: 8092\n",
      "number of catpions: 40460\n",
      "number of words: 9068\n"
     ]
    }
   ],
   "source": [
    "\"\"\" full captions to dictionary\n",
    "The dictionary has full dataset(training, validation, and test captions), \n",
    "and numbers are eliminated from all captions.\n",
    "Removing numbers improves performance (by about 3 points for bleu-1)\n",
    "\"\"\"\n",
    "\n",
    "captions = dict()\n",
    "words = set()\n",
    "\n",
    "with open(join(meta_info['text_dir'], 'Flickr8k.token.txt')) as f:\n",
    "    contents = f.read()\n",
    "    n_captions = 0\n",
    "    for line in contents.split('\\n'):\n",
    "        if line == '':\n",
    "            print(n_captions)\n",
    "            continue\n",
    "        if n_captions % 10000 == 0:\n",
    "            print(n_captions)\n",
    "        \n",
    "        file, caption = line.split('\\t')\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "        caption2 = []\n",
    "        for word in caption.split():\n",
    "            # remove number\n",
    "            if word.isalpha():\n",
    "                caption2.append(word.translate(table))\n",
    "        caption = ' '.join(caption2)\n",
    "        \n",
    "        img_id = file.split('.')[0]\n",
    "        \n",
    "        if img_id in captions.keys():\n",
    "            captions[img_id].append(caption)\n",
    "        else:\n",
    "            captions[img_id] = [caption]\n",
    "        n_captions += 1\n",
    "\n",
    "        [words.add(word) for word in caption.split()]\n",
    "        \n",
    "print('number of images: %d' % len(captions))\n",
    "print('number of catpions: %d' % n_captions)\n",
    "print('number of words: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A black dog is running after a white dog in the snow', 'Black dog chasing brown dog through snow', 'Two dogs chase each other across the snowy ground', 'Two dogs play together in the snow', 'Two dogs running through a low lying body of water']\n",
      "['the boy laying face down on a skateboard is being pushed along the ground by another boy', 'Two girls play on a skateboard in a courtyard', 'Two people play on a long skateboard', 'Two small children in red shirts playing on a skateboard', 'two young children on a skateboard going across a sidewalk']\n",
      "['The dogs are in the snow in front of a fence', 'The dogs play on the snow', 'Two brown dogs playfully fight in the snow', 'Two brown dogs wrestle in the snow', 'Two dogs playing in the snow']\n"
     ]
    }
   ],
   "source": [
    "# train set caption test\n",
    "print(captions['2513260012_03d33305cf'])\n",
    "# dev set caption test\n",
    "print(captions['2090545563_a4e66ec76b'])\n",
    "# test set caption test\n",
    "print(captions['3385593926_d3e9c21170'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "number of catpions: 1000\n",
      "number of catpions: 5000\n",
      "number of words: 3409\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Only dev captions are taken from the full captions set.\n",
    "Unlike above caption, this captions has sign of start and end for sequence.\n",
    "Each sssss, eeeee, but <> are not included in order to simplify the processing of the tokenizer.\n",
    "keras' tokenizer removes <>, so need to further processing in this process.\n",
    "\"\"\"\n",
    "\n",
    "dev_captions = dict()\n",
    "dev_words = set()\n",
    "\n",
    "with open(join(meta_info['text_dir'], 'Flickr_8k.devImages.txt')) as f:\n",
    "    contents = f.read()\n",
    "    n_dev_captions = 0\n",
    "    for line in contents.split('\\n'):\n",
    "        if line == '':\n",
    "            print(n_dev_captions)\n",
    "            continue\n",
    "        if n_dev_captions % 10000 == 0:\n",
    "            print(n_dev_captions)\n",
    "        \n",
    "        file = line.split('.')[0]\n",
    "        \n",
    "        for caption in captions[file]:\n",
    "            # start sign: sssss\n",
    "            # end sign: eeeee\n",
    "            caption = 'sssss ' + caption + ' eeeee'\n",
    "            caption = caption.replace('\\n', '')\n",
    "            \n",
    "            if file in dev_captions.keys():\n",
    "                dev_captions[file].append(caption)\n",
    "            else:\n",
    "                dev_captions[file] = [caption]\n",
    "            n_dev_captions += 1\n",
    "            \n",
    "            [dev_words.add(word) for word in caption.split()]\n",
    "\n",
    "print('number of catpions: %d' % len(dev_captions))\n",
    "print('number of catpions: %d' % n_dev_captions)\n",
    "print('number of words: %d' % len(dev_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sssss the boy laying face down on a skateboard is being pushed along the ground by another boy eeeee', 'sssss Two girls play on a skateboard in a courtyard eeeee', 'sssss Two people play on a long skateboard eeeee', 'sssss Two small children in red shirts playing on a skateboard eeeee', 'sssss two young children on a skateboard going across a sidewalk eeeee']\n"
     ]
    }
   ],
   "source": [
    "# dev set caption test\n",
    "print(dev_captions['2090545563_a4e66ec76b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "number of catpions: 6000\n",
      "number of catpions: 30000\n",
      "number of words: 7816\n",
      "max number of words in single sentence: 36\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unlike a dev set, training set must count the maximum number of words in single sentence.\n",
    "Variable M do that role.\n",
    "\"\"\"\n",
    "\n",
    "train_captions = dict()\n",
    "train_words = set()\n",
    "\n",
    "M = 0 # max length in single sentence\n",
    "\n",
    "with open(join(meta_info['text_dir'], 'Flickr_8k.trainImages.txt')) as f:\n",
    "    contents = f.read()\n",
    "    n_train_captions = 0\n",
    "    for line in contents.split('\\n'):\n",
    "        if line == '':\n",
    "            print(n_train_captions)\n",
    "            continue\n",
    "        if n_train_captions % 10000 == 0:\n",
    "            print(n_train_captions)\n",
    "        \n",
    "        file = line.split('.')[0]\n",
    "        \n",
    "        for caption in captions[file]:\n",
    "            caption = 'sssss ' + caption + ' eeeee'\n",
    "            caption = caption.replace('\\n', '')\n",
    "            \n",
    "            if file in train_captions.keys():\n",
    "                train_captions[file].append(caption)\n",
    "            else:\n",
    "                train_captions[file] = [caption]\n",
    "            n_train_captions += 1\n",
    "            \n",
    "            t = caption.split()\n",
    "            if len(t) > M:\n",
    "                M = len(t)\n",
    "            [train_words.add(word) for word in t]\n",
    "\n",
    "# n_vocabs = len(train_words) # all word, based str.split()\n",
    "\n",
    "print('number of catpions: %d' % len(train_captions))\n",
    "print('number of catpions: %d' % n_train_captions)\n",
    "print('number of words: %d' % len(train_words))\n",
    "\n",
    "# print('vocabulary size: %d' % n_vocabs)\n",
    "print('max number of words in single sentence: %d' % M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sssss A black dog is running after a white dog in the snow eeeee', 'sssss Black dog chasing brown dog through snow eeeee', 'sssss Two dogs chase each other across the snowy ground eeeee', 'sssss Two dogs play together in the snow eeeee', 'sssss Two dogs running through a low lying body of water eeeee']\n"
     ]
    }
   ],
   "source": [
    "# train set caption test\n",
    "print(train_captions['2513260012_03d33305cf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "number of catpions: 1000\n",
      "number of catpions: 5000\n",
      "number of words: 3266\n"
     ]
    }
   ],
   "source": [
    "test_captions = dict()\n",
    "test_words = set()\n",
    "\n",
    "with open(join(meta_info['text_dir'], 'Flickr_8k.testImages.txt')) as f:\n",
    "    contents = f.read()\n",
    "    n_test_captions = 0\n",
    "    for line in contents.split('\\n'):\n",
    "        if line == '':\n",
    "            print(n_test_captions)\n",
    "            continue\n",
    "        if n_test_captions % 10000 == 0:\n",
    "            print(n_test_captions)\n",
    "        \n",
    "        file = line.split('.')[0]\n",
    "        \n",
    "        for caption in captions[file]:\n",
    "            caption = 'sssss ' + caption + ' eeeee'\n",
    "            caption = caption.replace('\\n', '')\n",
    "            \n",
    "            if file in test_captions.keys():\n",
    "                test_captions[file].append(caption)\n",
    "            else:\n",
    "                test_captions[file] = [caption]\n",
    "            n_test_captions += 1\n",
    "            \n",
    "            [test_words.add(word) for word in caption.split()]\n",
    "\n",
    "print('number of catpions: %d' % len(test_captions))\n",
    "print('number of catpions: %d' % n_test_captions)\n",
    "print('number of words: %d' % len(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sssss The dogs are in the snow in front of a fence eeeee', 'sssss The dogs play on the snow eeeee', 'sssss Two brown dogs playfully fight in the snow eeeee', 'sssss Two brown dogs wrestle in the snow eeeee', 'sssss Two dogs playing in the snow eeeee']\n"
     ]
    }
   ],
   "source": [
    "# test set caption test\n",
    "print(test_captions['3385593926_d3e9c21170'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" make tokenizer using keras.\n",
    "Making tokenizer, only use train captions.\n",
    "\"\"\"\n",
    "def make_tokenizer(captions):\n",
    "    texts = []\n",
    "    for _, caption_list in captions.items():\n",
    "        for caption in caption_list:\n",
    "            texts.append(caption)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocabulary: 7277\n"
     ]
    }
   ],
   "source": [
    "tokenizer = make_tokenizer(train_captions)\n",
    "n_vocabs = len(tokenizer.word_index) + 1 # because index 0, plus 1\n",
    "print('number of vocabulary: %d' % n_vocabs)\n",
    "# print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "# print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "352425\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Make sequence, Make next word based ground truth.\n",
    "If single sentence consisting of N words, N + 1(because nd sign) sequences are created.\n",
    "Ex) Hi, I am a boy.\n",
    "sequence                 -> next word\n",
    "[]   []   []   []   [Hi] -> I\n",
    "[]   []   []   [Hi] [I]  -> am\n",
    "[]   []   [Hi] [I]  [am] -> a\n",
    "...\n",
    "[Hi] [I] [am] [a] [boy] -> eeeee(end sign)\n",
    "\"\"\"\n",
    "train_sequences = list()\n",
    "train_next_word = list()\n",
    "\n",
    "c = 0\n",
    "train_sequences_h5 = 'train_sequences.h5'\n",
    "train_next_word_h5 = 'train_next_word.h5'\n",
    "h5f1 = h5py.File(train_sequences_h5, 'w')\n",
    "h5f2 = h5py.File(train_next_word_h5, 'w')\n",
    "for img_id, captions in train_captions.items():\n",
    "#     print(img_id)\n",
    "    Xtrain = list()\n",
    "    ytrain = list()\n",
    "    for caption in captions:\n",
    "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
    "        \n",
    "        for i in range(1, len(sequence)): # except start sign\n",
    "            if c % 100000 == 0:\n",
    "                print(c)\n",
    "            train_sequences.append(pad_sequences([sequence[:i]], M)[0])\n",
    "            Xtrain.append(pad_sequences([sequence[:i]], M)[0])\n",
    "            train_next_word.append(to_categorical([sequence[i]], num_classes=n_vocabs)[0])\n",
    "            ytrain.append(to_categorical([sequence[i]], num_classes=n_vocabs)[0])\n",
    "            c += 1\n",
    "    h5f1.create_dataset(img_id, data=Xtrain)\n",
    "    h5f2.create_dataset(img_id, data=ytrain)\n",
    "h5f1.close()\n",
    "h5f2.close()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# test sequences and next word\n",
    "print(train_sequences[0])\n",
    "print(train_next_word[0])\n",
    "print(train_sequences[1])\n",
    "print(train_next_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "58661\n"
     ]
    }
   ],
   "source": [
    "dev_sequences = list()\n",
    "dev_next_word = list()\n",
    "\n",
    "c = 0\n",
    "dev_sequences_h5 = 'dev_sequences.h5'\n",
    "dev_next_word_h5 = 'dev_next_word.h5'\n",
    "h5f1 = h5py.File(dev_sequences_h5, 'w')\n",
    "h5f2 = h5py.File(dev_next_word_h5, 'w')\n",
    "for img_id, captions in dev_captions.items():\n",
    "#     print(img_id)\n",
    "    Xdev = list()\n",
    "    ydev = list()\n",
    "    for caption in captions:\n",
    "        text = tokenizer.texts_to_sequences([caption])[0]\n",
    "        \n",
    "        for i in range(1, len(text)):\n",
    "            if c % 10000 == 0:\n",
    "                print(c)\n",
    "            dev_sequences.append(pad_sequences([text[:i]], M)[0])\n",
    "            Xdev.append(pad_sequences([text[:i]], M)[0])\n",
    "            dev_next_word.append(to_categorical([text[i]], num_classes=n_vocabs)[0])\n",
    "            ydev.append(to_categorical([text[i]], num_classes=n_vocabs)[0])\n",
    "            c += 1\n",
    "    h5f1.create_dataset(img_id, data=Xdev)\n",
    "    h5f2.create_dataset(img_id, data=ydev)\n",
    "h5f1.close()\n",
    "h5f2.close()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "58389\n"
     ]
    }
   ],
   "source": [
    "test_sequences = list()\n",
    "test_next_word = list()\n",
    "\n",
    "c = 0\n",
    "test_sequences_h5 = 'test_sequences.h5'\n",
    "test_next_word_h5 = 'test_next_word.h5'\n",
    "h5f1 = h5py.File(test_sequences_h5, 'w')\n",
    "h5f2 = h5py.File(test_next_word_h5, 'w')\n",
    "for img_id, captions in test_captions.items():\n",
    "#     print(img_id)\n",
    "    Xtest = list()\n",
    "    ytest = list()\n",
    "    for caption in captions:\n",
    "        text = tokenizer.texts_to_sequences([caption])[0]\n",
    "        \n",
    "        for i in range(1, len(text)):\n",
    "            if c % 10000 == 0:\n",
    "                print(c)\n",
    "            test_sequences.append(pad_sequences([text[:i]], M)[0])\n",
    "            Xtest.append(pad_sequences([text[:i]], M)[0])\n",
    "            test_next_word.append(to_categorical([text[i]], num_classes=n_vocabs)[0])\n",
    "            ytest.append(to_categorical([text[i]], num_classes=n_vocabs)[0])\n",
    "            c += 1\n",
    "    h5f1.create_dataset(img_id, data=Xtest)\n",
    "    h5f2.create_dataset(img_id, data=ytest)\n",
    "h5f1.close()\n",
    "h5f2.close()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellow code isn't need to look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h5 -> Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = list()\n",
    "train_next_word = list()\n",
    "\n",
    "c = 0\n",
    "train_sequences_pkl = 'train_sequences.pkl'\n",
    "train_next_word_pkl = 'train_next_word.pkl'\n",
    "\n",
    "X = dict()\n",
    "Y = dict()\n",
    "\n",
    "for img_id, captions in train_captions.items():\n",
    "#     print(img_id)\n",
    "    Xtrain = list()\n",
    "    ytrain = list()\n",
    "    for caption in captions:\n",
    "        text = tokenizer.texts_to_sequences([caption])[0]\n",
    "        \n",
    "        for i in range(1, len(text)):\n",
    "            if c % 100000 == 0:\n",
    "                print(c)\n",
    "            train_sequences.append(pad_sequences([text[:i]], M)[0])\n",
    "            Xtrain.append(pad_sequences([text[:i]], M)[0])\n",
    "            train_next_word.append(to_categorical([text[i]], num_classes=n_vocabs)[0])\n",
    "            ytrain.append(to_categorical([text[i]], num_classes=n_vocabs)[0])\n",
    "            c += 1\n",
    "    X[img_id] = Xtrain\n",
    "    Y[img_id] = ytrain\n",
    "with open(train_sequences_pkl, 'wb') as f:\n",
    "    pickle.dump(X, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(train_next_word_pkl, 'wb') as f:\n",
    "    pickle.dump(Y, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_sequences_pkl, 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    print(test['2513260012_03d33305cf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8256\n",
      "8256\n"
     ]
    }
   ],
   "source": [
    "train_id_word = dict()\n",
    "\n",
    "for i, word in enumerate(train_words):\n",
    "    train_id_word[i] = word\n",
    "    train_word_id[word] = i\n",
    "\n",
    "print(len(train_id_word))\n",
    "print(len(train_word_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3523\n",
      "3523\n"
     ]
    }
   ],
   "source": [
    "dev_id_word = dict()\n",
    "dev_word_id = dict()\n",
    "\n",
    "for i, word in enumerate(dev_words):\n",
    "    dev_id_word[i] = word\n",
    "    dev_word_id[word] = i\n",
    "\n",
    "print(len(dev_id_word))\n",
    "print(len(dev_word_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "nextwords = list()\n",
    "\n",
    "data = {}\n",
    "for captions in train_captions.items():\n",
    "#     print(captions)\n",
    "    data[captions[0]] = []\n",
    "    for caption in captions[1]:\n",
    "        t = []\n",
    "        for word in caption.split():\n",
    "            t.append(train_word_id[word])\n",
    "        data[captions[0]].append(t)\n",
    "#     print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "383454\n"
     ]
    }
   ],
   "source": [
    "id_seq = {}\n",
    "id_y = {}\n",
    "c = 0\n",
    "for key, value in data.items():\n",
    "    sub_seqs = []\n",
    "    Y = []\n",
    "    for seq in value:\n",
    "        for i in range(1, len(seq)):\n",
    "            if c % 100000 == 0:\n",
    "                print(c)\n",
    "            sub_seqs.append(sequence.pad_sequences([seq[:i]], max_length)[0])\n",
    "            y = to_categorical([seq[i]], num_classes=n_vocab + 1)\n",
    "            Y.append(y[0])\n",
    "            c += 1\n",
    "            \n",
    "    id_seq[key] = sub_seqs\n",
    "    id_y[key] = Y\n",
    "print(c)\n",
    "#         print(id_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file_path = 'train_id_seq.h5'\n",
    "with h5py.File(h5file_path, 'w') as h5f:\n",
    "    for key, value in id_seq.items():\n",
    "        h5f.create_dataset(key, data=value)\n",
    "# print(feature_np)\n",
    "# np.squeeze(feature_np)\n",
    "# print(feature_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8133    0    0 ...    0    0    0]\n",
      " [8133 4381    0 ...    0    0    0]\n",
      " [8133 4381  850 ...    0    0    0]\n",
      " ...\n",
      " [8133 4752 3548 ...    0    0    0]\n",
      " [8133 4752 3548 ...    0    0    0]\n",
      " [8133 4752 3548 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "h5file_path = 'train_id_seq.h5'\n",
    "with h5py.File(h5file_path, 'r') as h5f:\n",
    "    print(h5f['667626_18933d713e'][:])\n",
    "# print(feature_np)\n",
    "# np.squeeze(feature_np)\n",
    "# print(feature_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file_path = 'train_id_y.h5'\n",
    "with h5py.File(h5file_path, 'w') as h5f:\n",
    "    for key, value in id_y.items():\n",
    "        h5f.create_dataset(key, data=value)\n",
    "# print(feature_np)\n",
    "# np.squeeze(feature_np)\n",
    "# print(feature_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "h5file_path = 'train_id_y.h5'\n",
    "with h5py.File(h5file_path, 'r') as h5f:\n",
    "    print(h5f['667626_18933d713e'][:])\n",
    "# print(feature_np)\n",
    "# np.squeeze(feature_np)\n",
    "# print(feature_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "nextwords = list()\n",
    "\n",
    "data = {}\n",
    "for captions in dev_captions.items():\n",
    "#     print(captions)\n",
    "    data[captions[0]] = []\n",
    "    for caption in captions[1]:\n",
    "        t = []\n",
    "        for word in caption.split():\n",
    "            t.append(dev_word_id[word])\n",
    "        data[captions[0]].append(t)\n",
    "#     print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "64445\n"
     ]
    }
   ],
   "source": [
    "id_seq = {}\n",
    "id_y = {}\n",
    "c = 0\n",
    "for key, value in data.items():\n",
    "    sub_seqs = []\n",
    "    Y = []\n",
    "    for seq in value:\n",
    "        for i in range(1, len(seq)):\n",
    "            if c % 10000 == 0:\n",
    "                print(c)\n",
    "            sub_seqs.append(sequence.pad_sequences([seq[:i]], max_length, padding='post')[0])\n",
    "            y = to_categorical([seq[i]], num_classes=n_vocab)\n",
    "            Y.append(y[0])\n",
    "            c += 1\n",
    "    id_seq[key] = sub_seqs\n",
    "    id_y[key] = Y\n",
    "print(c)\n",
    "#         print(id_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file_path = 'dev_id_seq.h5'\n",
    "with h5py.File(h5file_path, 'w') as h5f:\n",
    "    for key, value in id_seq.items():\n",
    "        h5f.create_dataset(key, data=value)\n",
    "# print(feature_np)\n",
    "# np.squeeze(feature_np)\n",
    "# print(feature_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1626    0    0 ...    0    0    0]\n",
      " [1626 1622    0 ...    0    0    0]\n",
      " [1626 1622  830 ...    0    0    0]\n",
      " ...\n",
      " [1626 3127 2829 ...    0    0    0]\n",
      " [1626 3127 2829 ...    0    0    0]\n",
      " [1626 3127 2829 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "h5file_path = 'dev_id_seq.h5'\n",
    "with h5py.File(h5file_path, 'r') as h5f:\n",
    "    print(h5f['2090545563_a4e66ec76b'][:])\n",
    "# print(feature_np)\n",
    "# np.squeeze(feature_np)\n",
    "# print(feature_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file_path = 'dev_id_y.h5'\n",
    "with h5py.File(h5file_path, 'w') as h5f:\n",
    "    for key, value in id_y.items():\n",
    "        h5f.create_dataset(key, data=value)\n",
    "# print(feature_np)\n",
    "# np.squeeze(feature_np)\n",
    "# print(feature_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "h5file_path = 'dev_id_y.h5'\n",
    "with h5py.File(h5file_path, 'r') as h5f:\n",
    "    print(h5f['2090545563_a4e66ec76b'][:])\n",
    "# print(feature_np)\n",
    "# np.squeeze(feature_np)\n",
    "# print(feature_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
