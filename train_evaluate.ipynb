{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, GRU\n",
    "from keras.layers.merge import add\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join, isdir, isfile, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = {\n",
    "    'input_shape': {\n",
    "        'VGG16': 4096, 'VGG19': 4096, 'ResNet50': 4096,\n",
    "        'InceptionV3': 2048,\n",
    "        'InceptionResNetV2': 1536\n",
    "    },\n",
    "    'n_embeddeing': 512,\n",
    "    'text_dir': 'Flickr8k_text/',\n",
    "    # It is better to input the value resulting from data processing.\n",
    "    # Automatic input at the stage of preparing the data, without having to input it.\n",
    "    'n_vocabs': None,\n",
    "    'M': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load data from dataset.\n",
    "Use squeezing to make it easier to put into model.\n",
    "\n",
    "X: image feature\n",
    "Y: caption sequence\n",
    "Z: caption sequence next word\n",
    "\"\"\"\n",
    "def load_data(dataset):\n",
    "    if dataset == 'train':\n",
    "        data_file = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "        features_file = 'train_features.h5'\n",
    "        sequences_file = 'train_sequences.h5'\n",
    "        next_word_file = 'train_next_word.h5'\n",
    "    elif dataset == 'dev':\n",
    "        data_file = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "        features_file = 'dev_features.h5'\n",
    "        sequences_file = 'dev_sequences.h5'\n",
    "        next_word_file = 'dev_next_word.h5'\n",
    "    elif dataset == 'test':\n",
    "        data_file = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "        features_file = 'test_features.h5'\n",
    "        sequences_file = 'test_sequences.h5'\n",
    "        next_word_file = 'test_next_word.h5'\n",
    "\n",
    "    features = h5py.File(features_file, 'r')\n",
    "    sequences = h5py.File(sequences_file, 'r')\n",
    "    next_word = h5py.File(next_word_file, 'r')\n",
    "\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    Z = list()\n",
    "\n",
    "    with open(data_file, 'r') as f:\n",
    "        contents = f.read()\n",
    "        c = 0\n",
    "        for line in contents.split('\\n'):\n",
    "            if line == '': # last line or error line\n",
    "                print(c)\n",
    "                continue\n",
    "\n",
    "            file = line.split('.')[0]\n",
    "\n",
    "            x = features[file][:]\n",
    "            y = sequences[file][:]\n",
    "            z = next_word[file][:]\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                if c % 10000 == 0:\n",
    "                    print(c)\n",
    "                X.append(x)\n",
    "                Y.append(y[i])\n",
    "                Z.append(z[i])\n",
    "                c += 1\n",
    "    features.close()\n",
    "    sequences.close()\n",
    "    next_word.close()\n",
    "\n",
    "    return np.array(X).squeeze(), np.array(Y).squeeze(), np.array(Z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "58661\n"
     ]
    }
   ],
   "source": [
    "x_dev, y_dev, z_dev = load_data('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58661, 4096)\n",
      "(4096,)\n",
      "(58661, 36)\n",
      "(36,)\n",
      "(58661, 7277)\n",
      "(7277,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Data load test\n",
    "k = 0\n",
    "print(x_dev.shape)\n",
    "print(x_dev[k].shape)\n",
    "print(y_dev.shape)\n",
    "print(y_dev[k].shape)\n",
    "print(z_dev.shape)\n",
    "print(z_dev[k].shape)\n",
    "\n",
    "print(y_dev[k])\n",
    "print(z_dev[k][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_embeddeing': 512, 'M': 36, 'n_vocabs': 7277, 'input_shape': {'VGG19': 4096, 'InceptionV3': 2048, 'InceptionResNetV2': 1536, 'VGG16': 4096, 'ResNet50': 4096}, 'text_dir': 'Flickr8k_text/'}\n"
     ]
    }
   ],
   "source": [
    "# Fill in meta info\n",
    "meta_info['M'] = y_dev.shape[1]\n",
    "meta_info['n_vocabs'] = z_dev.shape[1]\n",
    "print(meta_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I have tried various layer settings and optimizers, but the following model was the best.\n",
    "The paper refers to the LSTM size is set to 512,\n",
    "but in my experiment, the bleu score and learning speed are better when the LSTM size is 256 or 384.\n",
    "Adam optimizer is the best performance.\n",
    "\"\"\"\n",
    "def make_model(n_lstm_units):\n",
    "    # input1, input2 are encoder\n",
    "    # Image feature\n",
    "    input1 = Input(shape=(meta_info['input_shape']['VGG19'],))\n",
    "    dropout1 = Dropout(0.5)(input1)\n",
    "    fc1 = Dense(n_lstm_units, activation='relu')(dropout1)\n",
    "    \n",
    "    # Caption\n",
    "    input2 = Input(shape=(meta_info['M'],))\n",
    "    # In this paper, specified embedding vector size as 512.\n",
    "    embedded_layer1 = Embedding(meta_info['n_vocabs'], meta_info['n_embeddeing'], mask_zero=True)(input2)\n",
    "    dropout2 = Dropout(0.5)(embedded_layer1)\n",
    "    lstm1 = LSTM(n_lstm_units)(dropout2)\n",
    "    \n",
    "    # Decoder\n",
    "    fc2 = add([fc1, lstm1])\n",
    "    fc3 = Dense(n_lstm_units, activation='relu')(fc2)\n",
    "    outputs = Dense(meta_info['n_vocabs'], activation='softmax')(fc3)\n",
    "    \n",
    "    # Inputs are X, Y, and ouput is Z\n",
    "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU score test\n",
    "# ref: https://stackoverflow.com/questions/40542523/nltk-corpus-level-bleu-vs-sentence-level-bleu-score\n",
    "\n",
    "def bleu_score_test(model, test_captions, features, tokenizer):\n",
    "    y_true, y_pred = list(), list()\n",
    "    # step over the whole set\n",
    "    c = 0\n",
    "    for img_id, captions in test_captions.items():\n",
    "        if c % 100 == 0:\n",
    "            print(c)\n",
    "        # gererate caption\n",
    "        generated = generate_caption(model, tokenizer, features[img_id])\n",
    "        word_true = [caption.split() for caption in captions]\n",
    "        y_true.append(word_true)\n",
    "        y_pred.append(generated.split())\n",
    "        c += 1\n",
    "    # BLEU score test\n",
    "    print('BLEU-1: %f' % corpus_bleu(y_true, y_pred, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(y_true, y_pred, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(y_true, y_pred, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(y_true, y_pred, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "    # sentence_bleu equal score BLEU-4\n",
    "#     print('BLEU-sentence: %f' % sentence_bleu(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference by using the sampling method.(or k=1 Beam search)\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experimented with k=5 beam search, but the BLEU score is not much better from k=1.\n",
    "Also when k=1, the implementation is very simple.\n",
    "\"\"\"\n",
    "# Generate caption from input feature\n",
    "def generate_caption(model, tokenizer, feature):\n",
    "    # start sign\n",
    "    generated = '[CLS]'\n",
    "    # Loop for max length or end sign('[SEP]')\n",
    "    for i in range(meta_info['M']):\n",
    "        sequence = tokenizer.texts_to_sequences([generated])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=meta_info['M'])\n",
    "        # predict next word\n",
    "        y_pred = model.predict([feature, sequence], verbose=0)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "\n",
    "        word_pred = '[SEP]'\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i == y_pred:\n",
    "                word_pred = word \n",
    "        # Generate sentence\n",
    "        generated += ' ' + word_pred\n",
    "        # If end sign, break\n",
    "        if word_pred == '[SEP]':\n",
    "            break\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "352425\n",
      "(352425, 4096)\n",
      "(352425, 36)\n",
      "(352425, 7277)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, z_train = load_data('train')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(z_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 36, 512)      3725824     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 36, 512)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          787456      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7277)         1870189     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,498,093\n",
      "Trainable params: 7,498,093\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model(256)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 352425 samples, validate on 58661 samples\n",
      "Epoch 1/20\n",
      "352425/352425 [==============================] - 39s 109us/step - loss: 4.2600 - acc: 0.2664 - val_loss: 3.4729 - val_acc: 0.3412\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.34120, saving model to model.ep01.val_acc0.3412.h5\n",
      "Epoch 2/20\n",
      "352425/352425 [==============================] - 37s 104us/step - loss: 3.1648 - acc: 0.3665 - val_loss: 3.2255 - val_acc: 0.3679\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.34120 to 0.36793, saving model to model.ep02.val_acc0.3679.h5\n",
      "Epoch 3/20\n",
      "352425/352425 [==============================] - 37s 105us/step - loss: 2.8272 - acc: 0.3950 - val_loss: 3.1618 - val_acc: 0.3742\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.36793 to 0.37422, saving model to model.ep03.val_acc0.3742.h5\n",
      "Epoch 4/20\n",
      "352425/352425 [==============================] - 37s 105us/step - loss: 2.6127 - acc: 0.4154 - val_loss: 3.1523 - val_acc: 0.3833\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.37422 to 0.38329, saving model to model.ep04.val_acc0.3833.h5\n",
      "Epoch 5/20\n",
      "352425/352425 [==============================] - 37s 105us/step - loss: 2.4522 - acc: 0.4322 - val_loss: 3.1587 - val_acc: 0.3840\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.38329 to 0.38404, saving model to model.ep05.val_acc0.3840.h5\n",
      "Epoch 6/20\n",
      "352425/352425 [==============================] - 37s 106us/step - loss: 2.3308 - acc: 0.4462 - val_loss: 3.1750 - val_acc: 0.3853\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.38404 to 0.38530, saving model to model.ep06.val_acc0.3853.h5\n",
      "Epoch 7/20\n",
      "352425/352425 [==============================] - 37s 106us/step - loss: 2.2338 - acc: 0.4601 - val_loss: 3.1898 - val_acc: 0.3812\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.38530\n",
      "Epoch 8/20\n",
      "352425/352425 [==============================] - 37s 106us/step - loss: 2.1534 - acc: 0.4711 - val_loss: 3.2112 - val_acc: 0.3848\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.38530\n",
      "Epoch 9/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 2.0873 - acc: 0.4800 - val_loss: 3.2280 - val_acc: 0.3855\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.38530 to 0.38552, saving model to model.ep09.val_acc0.3855.h5\n",
      "Epoch 10/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 2.0294 - acc: 0.4883 - val_loss: 3.2518 - val_acc: 0.3824\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.38552\n",
      "Epoch 11/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.9768 - acc: 0.4971 - val_loss: 3.2959 - val_acc: 0.3826\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.38552\n",
      "Epoch 12/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.9311 - acc: 0.5048 - val_loss: 3.3054 - val_acc: 0.3808\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.38552\n",
      "Epoch 13/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.8912 - acc: 0.5110 - val_loss: 3.3383 - val_acc: 0.3809\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.38552\n",
      "Epoch 14/20\n",
      "352425/352425 [==============================] - 38s 106us/step - loss: 1.8537 - acc: 0.5174 - val_loss: 3.3691 - val_acc: 0.3803\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.38552\n",
      "Epoch 15/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.8168 - acc: 0.5248 - val_loss: 3.3780 - val_acc: 0.3812\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.38552\n",
      "Epoch 16/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.7844 - acc: 0.5305 - val_loss: 3.4020 - val_acc: 0.3822\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.38552\n",
      "Epoch 17/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.7577 - acc: 0.5348 - val_loss: 3.4362 - val_acc: 0.3754\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.38552\n",
      "Epoch 18/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.7306 - acc: 0.5412 - val_loss: 3.4504 - val_acc: 0.3772\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.38552\n",
      "Epoch 19/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.7058 - acc: 0.5449 - val_loss: 3.4739 - val_acc: 0.3770\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.38552\n",
      "Epoch 20/20\n",
      "352425/352425 [==============================] - 38s 107us/step - loss: 1.6785 - acc: 0.5512 - val_loss: 3.4962 - val_acc: 0.3767\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.38552\n"
     ]
    }
   ],
   "source": [
    "filepath = 'model.ep{epoch:02d}.val_acc{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit([x_train, y_train], z_train, epochs=20, batch_size=1024, verbose=1, callbacks=[checkpoint], validation_data=([x_dev, y_dev], z_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7276\n"
     ]
    }
   ],
   "source": [
    "# Load to tokenizer\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "40460\n",
      "number of images: 8092\n",
      "number of catpions: 40460\n",
      "number of words: 9068\n"
     ]
    }
   ],
   "source": [
    "captions = dict()\n",
    "words = set()\n",
    "\n",
    "with open(join(meta_info['text_dir'], 'Flickr8k.token.txt')) as f:\n",
    "    contents = f.read()\n",
    "    n_captions = 0\n",
    "    for line in contents.split('\\n'):\n",
    "        if line == '':\n",
    "            print(n_captions)\n",
    "            continue\n",
    "        if n_captions % 10000 == 0:\n",
    "            print(n_captions)\n",
    "        \n",
    "        file, caption = line.split('\\t')\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "        caption2 = []\n",
    "        for word in caption.split():\n",
    "            if word.isalpha():\n",
    "                caption2.append(word.translate(table))\n",
    "        caption = ' '.join(caption2)\n",
    "        \n",
    "        img_id = file.split('.')[0]\n",
    "        \n",
    "        if img_id in captions.keys():\n",
    "            captions[img_id].append(caption)\n",
    "        else:\n",
    "            captions[img_id] = [caption]\n",
    "        n_captions += 1\n",
    "\n",
    "        [words.add(word) for word in caption.split()]\n",
    "        \n",
    "print('number of images: %d' % len(captions))\n",
    "print('number of catpions: %d' % n_captions)\n",
    "print('number of words: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A black dog is running after a white dog in the snow', 'Black dog chasing brown dog through snow', 'Two dogs chase each other across the snowy ground', 'Two dogs play together in the snow', 'Two dogs running through a low lying body of water']\n",
      "['the boy laying face down on a skateboard is being pushed along the ground by another boy', 'Two girls play on a skateboard in a courtyard', 'Two people play on a long skateboard', 'Two small children in red shirts playing on a skateboard', 'two young children on a skateboard going across a sidewalk']\n",
      "['The dogs are in the snow in front of a fence', 'The dogs play on the snow', 'Two brown dogs playfully fight in the snow', 'Two brown dogs wrestle in the snow', 'Two dogs playing in the snow']\n"
     ]
    }
   ],
   "source": [
    "# train set caption test\n",
    "print(captions['2513260012_03d33305cf'])\n",
    "# dev set caption test\n",
    "print(captions['2090545563_a4e66ec76b'])\n",
    "# test set caption test\n",
    "print(captions['3385593926_d3e9c21170'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "number of images: 1000\n",
      "number of catpions: 5000\n"
     ]
    }
   ],
   "source": [
    "test_captions = dict()\n",
    "\n",
    "with open('Flickr8k_text/Flickr_8k.testImages.txt', 'r') as f:\n",
    "    contents = f.read()\n",
    "    n_captions = 0\n",
    "    for line in contents.split('\\n'):\n",
    "        if line == '':\n",
    "            print(n_captions)\n",
    "            continue\n",
    "        if n_captions % 1000 == 0:\n",
    "            print(n_captions)\n",
    "            \n",
    "        file = line\n",
    "        img_id = file.split('.')[0]\n",
    "        test_captions[img_id] = []\n",
    "\n",
    "        for caption in captions[img_id]:\n",
    "            caption = '[CLS] ' + caption + ' [SEP]'\n",
    "            caption = caption.replace('\\n', '')\n",
    "            test_captions[img_id].append(caption)\n",
    "        n_captions += len(captions[img_id])\n",
    "        \n",
    "print('number of images: %d' % len(test_captions))\n",
    "print('number of catpions: %d' % n_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sssss The dogs are in the snow in front of a fence eeeee', 'sssss The dogs play on the snow eeeee', 'sssss Two brown dogs playfully fight in the snow eeeee', 'sssss Two brown dogs wrestle in the snow eeeee', 'sssss Two dogs playing in the snow eeeee']\n"
     ]
    }
   ],
   "source": [
    "# test set caption test\n",
    "print(test_captions['3385593926_d3e9c21170'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "BLEU-1: 0.594751\n",
      "BLEU-2: 0.361690\n",
      "BLEU-3: 0.258544\n",
      "BLEU-4: 0.138500\n"
     ]
    }
   ],
   "source": [
    "# BLEU scores test for base model\n",
    "test_features_file = 'test_features.h5'\n",
    "model_file = 'model.ep06.val_acc0.3853.h5'\n",
    "with h5py.File(test_features_file, 'r') as h5f:\n",
    "    test_features = h5f\n",
    "    model = load_model(model_file)\n",
    "    bleu_score_test(model, test_captions, test_features, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "58389\n",
      "(58389, 4096)\n",
      "(58389, 36)\n",
      "(58389, 7277)\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test, z_test = load_data('test')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(z_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58661 samples, validate on 58389 samples\n",
      "Epoch 1/1\n",
      "58661/58661 [==============================] - 9s 160us/step - loss: 3.0794 - acc: 0.3872 - val_loss: 3.0183 - val_acc: 0.3945\n",
      "\n",
      "Epoch 00001: saving model to transfer.model.ep001.acc0.3872.h5\n"
     ]
    }
   ],
   "source": [
    "filepath = 'transfer.model.ep{epoch:03d}.acc{acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1)\n",
    "\n",
    "model = load_model('model.ep06.val_acc0.3853.h5')\n",
    "# transfer learning, train -> dev, dev -> test\n",
    "history = model.fit([x_dev, y_dev], z_dev, epochs=1, batch_size=1024, verbose=1, callbacks=[checkpoint], validation_data=([x_test, y_test], z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "BLEU-1: 0.611284\n",
      "BLEU-2: 0.370745\n",
      "BLEU-3: 0.262829\n",
      "BLEU-4: 0.136760\n"
     ]
    }
   ],
   "source": [
    "test_features_file = 'test_features.h5'\n",
    "model_file = 'transfer.model.ep001.acc0.3872.h5'\n",
    "with h5py.File(test_features_file, 'r') as h5f:\n",
    "    test_features = h5f\n",
    "    model = load_model(model_file)\n",
    "    bleu_score_test(model, test_captions, test_features, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
